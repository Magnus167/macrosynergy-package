{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation: k-fold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of testing is to estimate a models quality of predicting data out of sample. For the purpose of testing a single split of the data has greater risk of of not being representative for a model's ability to generalize. Hence, where possible, multiple splits are preferred. An unordered dataset is typically split into folds. <u>Out of k folds each one is used as test set in turn</u>. There are as many train-test splits and scores as there are folds.\n",
    "\n",
    "This maximizes the amount of data that is used to train the model, as during the course of training, the model is not only trained, but also tested on all of the available data. However, <u>the number of folds also determines the computational cost</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.63919994  0.71386698  0.58702344  0.07923081 -0.25294154]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "boston_X, boston_y = datasets.load_boston(return_X_y=True)\n",
    "\n",
    "reg = LinearRegression()\n",
    "cv_results = cross_val_score(reg, boston_X, boston_y, cv=5)  # gives array of R2s\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters are parameters that cannot be learned by fitting a model. Tuning refers to methods for setting these parameters before fitting a model. Specifically, hyperparameter tuning makes choices based on the success of the related model.\n",
    "\n",
    "A basic method is grid search. A <u>grid refers to combinations of plausible hyperparameter values</u>. The combination is then determined through a 'grid search'. The `sklearn` documentation shows the names of each model's hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: Tuning knn classifier with `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_iris(return_X_y=True) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, stratify=y)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'n_neighbours': np.arange(1, 50)}  # grid is specified as dictionary of key-range pairs\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn_cv = GridSearchCV(knn, param_grid, cv=5)  # creates grid search object\n",
    "# knn_cv.fit(X_train, y_train)  # fit performs the actual grid search in place\n",
    "# knn_cv.best_params_  # return the most successful hyperparameters\n",
    "# knn_cv.best_score_  # return the score (here accuracy) of the most successful hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 0, 2, 0, 1, 2, 1, 0, 2, 0, 1, 1, 2, 1, 2, 1, 1, 1, 2, 0,\n",
       "       1, 1, 1, 0, 1, 0, 2, 2, 1, 1, 2, 2, 2, 0, 0, 1, 0, 1, 1, 0, 1, 2,\n",
       "       2, 1, 0, 1, 2, 2, 0, 0, 0, 2, 2, 1, 2, 2, 1, 2, 0, 2, 1, 2, 0, 2,\n",
       "       0, 0, 1, 2, 0, 0, 1, 0, 2, 0, 2, 0, 0, 1, 2, 0, 2, 2, 1, 2, 0, 2,\n",
       "       1, 0, 0, 0, 0, 2, 1, 2, 1, 1, 1, 0, 1, 0, 0, 2, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tryouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/tutorial/index.html#tutorial-menu\n",
    "\n",
    "now:\n",
    "https://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
