{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation: k-fold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of testing is to estimate a models quality of predicting data out of sample. A single train-test split of the data bears the risk of not being representative for a model's ability to generalize. Hence, where possible, multiple splits are preferred. An unordered dataset is typically split into folds. <u>Out of k folds each one is used as test set in turn</u>. There are as many train-test splits and scores as there are folds.\n",
    "\n",
    "This maximizes the amount of data that is used to train the model, as during the course of training, the model is not only trained, but also tested on all of the available data. However, <u>the number of folds also determines the computational cost</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.63919994  0.71386698  0.58702344  0.07923081 -0.25294154]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "boston_X, boston_y = datasets.load_boston(return_X_y=True)\n",
    "\n",
    "reg = LinearRegression()\n",
    "cv_results = cross_val_score(reg, boston_X, boston_y, cv=5)  # gives array of R2s\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Hyperparameters are parameters that cannot be directley learned by fitting a model. Hyperparameter tuning chooses these parameters based on the success of the related model.\n",
    "\n",
    "In scikit-learn hyperparameters are passed as arguments to the constructor of the estimator class. Examples include `C`, `kernel` and `gamma` for Support Vector Classifier and `alpha` (coefficient penalty) for Lasso.\n",
    "\n",
    "One can search the hyper-parameter space for the best cross validation score. Any parameter provided when constructing an estimator may be optimized in this manner. To find names and values of all parameters for a given estimator one can use `estimator.get_params()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'auto',\n",
       " 'leaf_size': 30,\n",
       " 'metric': 'minkowski',\n",
       " 'metric_params': None,\n",
       " 'n_jobs': None,\n",
       " 'n_neighbors': 5,\n",
       " 'p': 2,\n",
       " 'weights': 'uniform'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic method for hyperparameter optimization is grid search. A <u>grid refers to combinations of plausible hyperparameter values</u>. The combination is then determined through a 'grid search'. The `sklearn` documentation shows the names of each model's hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1**: Find in-sample optimal number of neighbors for knn classifier with `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 3}\n",
      "0.9714285714285715\n"
     ]
    }
   ],
   "source": [
    "X, y = datasets.load_iris(return_X_y=True) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, stratify=y)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_neighbors': np.arange(1, 50)}  # grid is specified as dictionary of key-range pairs\n",
    "knn = KNeighborsClassifier()  # initiate estimator\n",
    "\n",
    "knn_cv = GridSearchCV(knn, param_grid, cv=5)  # initiate grid search object\n",
    "knn_cv.fit(X_train, y_train)  # fit performs the actual grid search in place\n",
    "print(knn_cv.best_params_)  # return the most successful hyperparameters\n",
    "print(knn_cv.best_score_)  # return the score (here accuracy) of the most successful hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2**: Find historically optimal regularization parameter for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'C': 31.622776601683793}\n",
      "Best score is 0.9800000000000001\n"
     ]
    }
   ],
   "source": [
    "X, y = datasets.load_iris(return_X_y=True) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, stratify=y)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': np.logspace(-5, 8, 15)}  # Setup the hyperparameter grid\n",
    "logreg = LogisticRegression()  # instantiate logistic regression classifier\n",
    "\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)  # instantiate the GridSearchCV object\n",
    "logreg_cv.fit(X, y)  # fit it to the data\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning should split out a test or **hold-out set** before the tuning. The hold-out set serves as basis for testing the predictive quality of the overall method we used, i.e. hypterparameter tuning and estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameter: {'C': 3.727593720314938, 'penalty': 'l2'}\n",
      "Tuned Logistic Regression Accuracy: 0.97\n",
      "Test set accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "X, y = datasets.load_iris(return_X_y=True) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, stratify=y)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': np.logspace(-5, 8, 15), 'penalty': ['l1', 'l2']}  # Create the hyperparameter grid\n",
    "logreg = LogisticRegression()  # instantiate the logistic regression classifier\n",
    "\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)  # instantiate GridSearchCV object\n",
    "logreg_cv.fit(X_train, y_train)  # fit it to the training data to hyperparameter tuning\n",
    "\n",
    "print(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\n",
    "print(\"Tuned Logistic Regression Accuracy: {:.2f}\".format(logreg_cv.best_score_))\n",
    "\n",
    "y_pred = logreg_cv.predict(X_test)  # predict for the holdout test set\n",
    "logreg_cv.score(X_test, y_test)  # checkout test accuracy\n",
    "print(\"Test set accuracy: {:.2f}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV can be computationally too expensive when searching over a large multi-variate hyperparameter spaces. Randomized search with `RandomizedSearchCV` is a \"cheaper\" solution in which not all hyperparameter values are tried out. Instead, a fixed number of hyperparameter settings is sampled from specified probability distributions. \n",
    "\n",
    "For example, decision trees have many parameters that can be tuned, such as `max_features`, `max_depth`, and `min_samples_leaf`. This makes it an ideal use case for randomized search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': 3, 'max_features': 3, 'min_samples_leaf': 3}\n",
      "Best score is 0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {\"max_depth\": [3, None],  # set up the parameters and distributions to sample from\n",
    "              \"max_features\": randint(1, 9),\n",
    "              \"min_samples_leaf\": randint(1, 9),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "tree = DecisionTreeClassifier()  # instantiate a Decision Tree classifier\n",
    "tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)  # instantiate the RandomizedSearchCV object\n",
    "tree_cv.fit(X, y)  # fit the RSCV object to the data\n",
    "\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {}\".format(tree_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tryouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/tutorial/index.html#tutorial-menu\n",
    "\n",
    "now:\n",
    "https://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
