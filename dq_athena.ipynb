{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0809026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from collections import defaultdict\n",
    "import yaml\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e66033",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://platform.jpmorgan.com/research/dataquery/api/v2\"\n",
    "class DataQueryInterface(object):\n",
    "\n",
    "    def __init__(self, username: str, password: str,\n",
    "                 crt: str = \"api_macrosynergy_com.crt\",\n",
    "                 key: str = \"api_macrosynergy_com.key\"):\n",
    "\n",
    "        self.auth = base64.b64encode(bytes(f'{username:s}:{password:s}',\n",
    "                                           \"utf-8\")).decode('ascii')\n",
    "        self.headers = {\"Authorization\": f\"Basic {self.auth:s}\"}\n",
    "        self.base_url = URL\n",
    "        self.cert = (crt, key)\n",
    "\n",
    "    def _fetch_ts(self, params: dict, start_date: str = None, end_date: str = None,\n",
    "                  calendar: str = \"CAL_ALLDAYS\", frequency: str = \"FREQ_DAY\"):\n",
    "\n",
    "        params[\"format\"] = \"JSON\"\n",
    "        params[\"start-date\"] = start_date\n",
    "        params[\"end-date\"] = end_date\n",
    "        params[\"calendar\"] = calendar\n",
    "        params[\"frequency\"] = frequency\n",
    "\n",
    "        endpoint = \"/expressions/time-series\"\n",
    "        url = self.base_url + endpoint\n",
    "        results = []\n",
    "\n",
    "        with requests.get(url=url, cert=self.cert, headers=self.headers,\n",
    "                          params=params) as r:\n",
    "            self.last_response = r.text\n",
    "        response = json.loads(self.last_response)\n",
    "\n",
    "        assert \"instruments\" in response.keys()\n",
    "        results.extend(response[\"instruments\"])\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_tickers(self, tickers, original_metrics, **kwargs):\n",
    "        \n",
    "        unique_tix = list(set(tickers))\n",
    "        dq_tix = []\n",
    "        for metric in original_metrics:\n",
    "            dq_tix += [\"DB(JPMAQS,\" + tick + f\",{metric})\" for tick in unique_tix]\n",
    "\n",
    "        tickers = dq_tix\n",
    "\n",
    "        no_tickers = len(tickers)\n",
    "        iterations = ceil(no_tickers / 20)\n",
    "        remainder = no_tickers % 20\n",
    "\n",
    "        results = []\n",
    "        tickers_copy = tickers.copy()\n",
    "        for i in range(iterations):\n",
    "            if i < (iterations - 1):\n",
    "                tickers = tickers_copy[i * 20: (i * 20) + 20]\n",
    "            else:\n",
    "                tickers = tickers_copy[-remainder:]\n",
    "\n",
    "            params = {\"expressions\": tickers}\n",
    "            output = self._fetch_ts(params=params, **kwargs)\n",
    "            results.extend(output)\n",
    "\n",
    "        no_metrics = len(set([tick.split(',')[-1][:-1] for tick in tickers_copy]))\n",
    "\n",
    "        output_dict = self.isolate_timeseries(results, original_metrics)\n",
    "        output_dict = self.valid_ticker(output_dict, original_metrics)\n",
    "        \n",
    "        df_athena = self.df_column(output_dict, original_metrics)\n",
    "        \n",
    "        return self.standardise(df_athena)\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def isolate_timeseries(list_, metrics):\n",
    "\n",
    "        output_dict = defaultdict(dict)\n",
    "        size = len(list_)\n",
    "\n",
    "        for i in range(size):\n",
    "            try:\n",
    "                r = list_.pop()\n",
    "            except IndexError:\n",
    "                break\n",
    "            else:\n",
    "                dictionary = r['attributes'][0]\n",
    "                ticker = dictionary['expression'].split(',')\n",
    "                metric = ticker[-1][:-1]\n",
    "\n",
    "                ticker_split = ','.join(ticker[:-1])\n",
    "                ts_arr = np.array(dictionary['time-series'])\n",
    "\n",
    "                if ticker_split not in output_dict:\n",
    "                    output_dict[ticker_split]['real_date'] = ts_arr[:, 0]\n",
    "                    output_dict[ticker_split][metric] = ts_arr[:, 1]\n",
    "                else:\n",
    "                    output_dict[ticker_split][metric] = ts_arr[:, 1]\n",
    "        \n",
    "        return output_dict\n",
    "    \n",
    "    @staticmethod\n",
    "    def column_check(v, metric):\n",
    "        \n",
    "        data = list(v[metric])\n",
    "        condition = all([isinstance(elem, type(None)) for elem in data])\n",
    "\n",
    "        return condition\n",
    "\n",
    "    def valid_ticker(self, _dict, original_metrics):\n",
    "        \n",
    "        metric = next(iter(original_metrics))\n",
    "        \n",
    "        dict_copy = _dict.copy()\n",
    "        for k, v in _dict.items():\n",
    "\n",
    "            condition = self.column_check(v, metric)\n",
    "            if condition:\n",
    "\n",
    "                print(f\"The ticker, {k + ')'}, does not exist in the Database.\")\n",
    "                dict_copy.pop(k)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        return dict_copy\n",
    "    \n",
    "    @staticmethod\n",
    "    def df_column(output_dict, original_metrics):\n",
    "\n",
    "        index = next(iter(output_dict.values()))['real_date']\n",
    "        no_rows = index.size\n",
    "        no_columns = len(output_dict.keys()) * len(original_metrics)\n",
    "        arr = np.empty(shape=(no_rows, no_columns), dtype=np.float32)\n",
    "\n",
    "        i = 0\n",
    "        columns = []\n",
    "        for metric in original_metrics:\n",
    "            for k, v in output_dict.items():\n",
    "\n",
    "                col_name = k + ',' + metric + ')'\n",
    "                columns.append(col_name)\n",
    "                arr[:, i] = v[metric]\n",
    "                i += 1\n",
    "\n",
    "        df = pd.DataFrame(data=arr, columns=columns, index=index)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def standardise(self, df_athena):\n",
    "        \n",
    "        columns = list(df_athena.columns)\n",
    "        index = df_athena.index\n",
    "        dates = list(map(lambda d: datetime.strptime(d, \"%Y%m%d\"), index))\n",
    "        \n",
    "        metrics = set(map(lambda ticker: ticker.split(',')[-1][:-1], columns))\n",
    "        metrics = list(metrics)\n",
    "        \n",
    "        output_columns = ['cid', 'xcat', 'real_date'] + metrics\n",
    "        \n",
    "        shape = df_athena.shape\n",
    "        no_metrics = len(metrics)\n",
    "        no_cids = int(shape[1] / no_metrics)\n",
    "        \n",
    "        no_rows = shape[0] * no_cids\n",
    "        arr = np.empty(shape = (no_rows, len(output_columns)), dtype=object)\n",
    "        \n",
    "        dict_ = defaultdict(list)\n",
    "\n",
    "        for col in range(df_athena.shape[1]):\n",
    "            \n",
    "            col_name = str(df_athena.iloc[:, col].name)\n",
    "            ticker = col_name.split(',')[1]\n",
    "            dict_[ticker].append(df_athena.iloc[:, col])\n",
    "        \n",
    "        no_dates = len(dates)\n",
    "        i = 0\n",
    "        for k, v in dict_.items():\n",
    "            \n",
    "            cid = k[:3]\n",
    "            v.insert(0, np.repeat(cid, no_dates))\n",
    "            v.insert(1, np.repeat(k[4:], no_dates))\n",
    "            v.insert(2, dates)\n",
    "            data = np.column_stack(tuple(v))\n",
    "            \n",
    "            arr[i * no_dates:no_dates * (i + 1), :] = data\n",
    "            \n",
    "            i += 1\n",
    "                     \n",
    "        return pd.DataFrame(data = arr, columns = output_columns)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21331e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dq_output(tickers=None, metrics=['value'], start_date='2000-01-01'):\n",
    "\n",
    "    with open(\"config.yml\", 'r') as f:\n",
    "        cf = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    dq = DataQueryInterface(username=cf[\"dq\"][\"username\"], password=cf[\"dq\"][\"password\"],\n",
    "                            crt=\"api_macrosynergy_com.crt\",\n",
    "                            key=\"api_macrosynergy_com.key\")\n",
    "\n",
    "    df_ts = dq.get_tickers(tickers=tickers, original_metrics=metrics, start_date=start_date)\n",
    "\n",
    "    return df_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9869c007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ticker, DB(JPMAQS,AUD_DU05YXR_NSA), does not exist in the Database.\n",
      "The ticker, DB(JPMAQS,ESP_DU05YXR_NSA), does not exist in the Database.\n",
      "The ticker, DB(JPMAQS,ESP_CPIXFE_SJA_P6M6ML6AR), does not exist in the Database.\n",
      "    cid                  xcat  real_date   eop_lag  value mop_lag\n",
      "0   CAD  CPIXFE_SJA_P6M6ML6AR 2021-11-01  3.323818   32.0   214.0\n",
      "1   CAD  CPIXFE_SJA_P6M6ML6AR 2021-11-02  3.323818   33.0   215.0\n",
      "2   CAD  CPIXFE_SJA_P6M6ML6AR 2021-11-03  3.323818   34.0   216.0\n",
      "3   CAD  CPIXFE_SJA_P6M6ML6AR 2021-11-04  3.323818   35.0   217.0\n",
      "4   CAD  CPIXFE_SJA_P6M6ML6AR 2021-11-05  3.323818   36.0   218.0\n",
      "5   CAD  CPIXFE_SJA_P6M6ML6AR 2021-11-06       NaN    NaN     NaN\n",
      "6   CAD  CPIXFE_SJA_P6M6ML6AR 2021-11-07       NaN    NaN     NaN\n",
      "7   CAD  CPIXFE_SJA_P6M6ML6AR 2021-11-08  3.323818   39.0   221.0\n",
      "8   CAD  CPIXFE_SJA_P6M6ML6AR 2021-11-09  3.323818   40.0   222.0\n",
      "9   CAD  CPIXFE_SJA_P6M6ML6AR 2021-11-10  3.323818   41.0   223.0\n",
      "10  CAD  CPIXFE_SJA_P6M6ML6AR 2021-11-11  3.323818   42.0   224.0\n",
      "11  CAD  CPIXFE_SJA_P6M6ML6AR 2021-11-12  3.323818   43.0   225.0\n",
      "12  CAD  CPIXFE_SJA_P6M6ML6AR 2021-11-13       NaN    NaN     NaN\n",
      "13  CAD  CPIXFE_SJA_P6M6ML6AR 2021-11-14       NaN    NaN     NaN\n",
      "14  CAD  CPIXFE_SJA_P6M6ML6AR 2021-11-15  3.323818   46.0   228.0\n",
      "15  CAD           DU05YXR_NSA 2021-11-01  0.046398    0.0     0.0\n",
      "16  CAD           DU05YXR_NSA 2021-11-02  0.073999    0.0     0.0\n",
      "17  CAD           DU05YXR_NSA 2021-11-03       NaN    NaN     NaN\n",
      "18  CAD           DU05YXR_NSA 2021-11-04       NaN    NaN     NaN\n",
      "19  CAD           DU05YXR_NSA 2021-11-05       NaN    NaN     NaN\n",
      "20  CAD           DU05YXR_NSA 2021-11-06       NaN    NaN     NaN\n",
      "21  CAD           DU05YXR_NSA 2021-11-07       NaN    NaN     NaN\n",
      "22  CAD           DU05YXR_NSA 2021-11-08       NaN    NaN     NaN\n",
      "23  CAD           DU05YXR_NSA 2021-11-09       NaN    NaN     NaN\n",
      "24  CAD           DU05YXR_NSA 2021-11-10       NaN    NaN     NaN\n",
      "25  CAD           DU05YXR_NSA 2021-11-11       NaN    NaN     NaN\n",
      "26  CAD           DU05YXR_NSA 2021-11-12       NaN    NaN     NaN\n",
      "27  CAD           DU05YXR_NSA 2021-11-13       NaN    NaN     NaN\n",
      "28  CAD           DU05YXR_NSA 2021-11-14       NaN    NaN     NaN\n",
      "29  CAD           DU05YXR_NSA 2021-11-15       NaN    NaN     NaN\n",
      "30  AUD  CPIXFE_SJA_P6M6ML6AR 2021-11-01  1.820391  124.0   306.0\n",
      "31  AUD  CPIXFE_SJA_P6M6ML6AR 2021-11-02  1.820391  125.0   307.0\n",
      "32  AUD  CPIXFE_SJA_P6M6ML6AR 2021-11-03  1.820391  126.0   308.0\n",
      "33  AUD  CPIXFE_SJA_P6M6ML6AR 2021-11-04  1.820391  127.0   309.0\n",
      "34  AUD  CPIXFE_SJA_P6M6ML6AR 2021-11-05  1.820391  128.0   310.0\n",
      "35  AUD  CPIXFE_SJA_P6M6ML6AR 2021-11-06       NaN    NaN     NaN\n",
      "36  AUD  CPIXFE_SJA_P6M6ML6AR 2021-11-07       NaN    NaN     NaN\n",
      "37  AUD  CPIXFE_SJA_P6M6ML6AR 2021-11-08  1.820391  131.0   313.0\n",
      "38  AUD  CPIXFE_SJA_P6M6ML6AR 2021-11-09  1.820391  132.0   314.0\n",
      "39  AUD  CPIXFE_SJA_P6M6ML6AR 2021-11-10  1.820391  133.0   315.0\n",
      "40  AUD  CPIXFE_SJA_P6M6ML6AR 2021-11-11  1.820391  134.0   316.0\n",
      "41  AUD  CPIXFE_SJA_P6M6ML6AR 2021-11-12  1.820391  135.0   317.0\n",
      "42  AUD  CPIXFE_SJA_P6M6ML6AR 2021-11-13       NaN    NaN     NaN\n",
      "43  AUD  CPIXFE_SJA_P6M6ML6AR 2021-11-14       NaN    NaN     NaN\n",
      "44  AUD  CPIXFE_SJA_P6M6ML6AR 2021-11-15  1.820391  138.0   320.0\n"
     ]
    }
   ],
   "source": [
    "metrics = ['value', 'eop_lag', 'mop_lag']\n",
    "xcats = ['CPIXFE_SJA_P6M6ML6AR', 'DU05YXR_NSA']\n",
    "cids = ['AUD', 'CAD', 'ESP']\n",
    "tickers = [cid + '_' + xcat for xcat in xcats for cid in cids]\n",
    "\n",
    "df = dq_output(tickers = tickers, metrics=metrics, start_date=\"2021-11-01\")\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
