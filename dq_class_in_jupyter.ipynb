{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8ea9b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from collections import defaultdict\n",
    "import yaml\n",
    "import base64\n",
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b6b90a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQueryInterface(object):\n",
    "\n",
    "    def __init__(self, username: str, password: str,\n",
    "                 crt: str = \"api_macrosynergy_com.crt\",\n",
    "                 key: str = \"api_macrosynergy_com.key\"):\n",
    "\n",
    "        self.auth = base64.b64encode(bytes(f'{username:s}:{password:s}',\n",
    "                                           \"utf-8\")).decode('ascii')\n",
    "        self.headers = {\"Authorization\": f\"Basic {self.auth:s}\"}\n",
    "        self.base_url = \"https://platform.jpmorgan.com/research/dataquery/api/v2\"\n",
    "        self.cert = (crt, key)\n",
    "\n",
    "    def _fetch(self, endpoint: str = \"/groups\", select: str = \"groups\",\n",
    "               params: dict = None):\n",
    "        url = self.base_url + endpoint\n",
    "        results = []\n",
    "\n",
    "        with requests.get(url=url, cert=self.cert, headers=self.headers, params=params) \\\n",
    "                as r:\n",
    "            self.last_response = r.text\n",
    "        response = json.loads(self.last_response)\n",
    "\n",
    "        assert select in response.keys()\n",
    "        results.extend(response[select])\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _fetch_ts(self, endpoint: str, params: dict, start_date: str = None,\n",
    "                  end_date: str = None, calendar: str = \"CAL_ALLDAYS\",\n",
    "                  frequency: str = \"FREQ_DAY\"):\n",
    "\n",
    "        params[\"format\"] = \"JSON\"\n",
    "        params[\"start-date\"] = start_date\n",
    "        params[\"end-date\"] = end_date\n",
    "        params[\"calendar\"] = calendar\n",
    "        params[\"frequency\"] = frequency\n",
    "        results = self._fetch(endpoint=endpoint, params=params, select=\"instruments\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_tickers(self, tickers, original_metrics, **kwargs):\n",
    "\n",
    "        no_tickers = len(tickers)\n",
    "        iterations = ceil(no_tickers / 20)\n",
    "        remainder = no_tickers % 20\n",
    "\n",
    "        results = []\n",
    "        tickers_copy = tickers.copy()\n",
    "        for i in range(iterations):\n",
    "            if i < (iterations - 1):\n",
    "                tickers = tickers_copy[i * 20: (i * 20) + 20]\n",
    "            else:\n",
    "                tickers = tickers_copy[-remainder:]\n",
    "\n",
    "            params = {\"expressions\": tickers}\n",
    "            output = self._fetch_ts(endpoint=\"/expressions/time-series\",\n",
    "                                    params=params, **kwargs)\n",
    "            results.extend(output)\n",
    "\n",
    "        no_metrics = len(set([tick.split(',')[-1][:-1] for tick in tickers_copy]))\n",
    "        print(f\"Number of metrics: {no_metrics}.\")\n",
    "\n",
    "        results_dict = self.isolate_timeseries(results, original_metrics)\n",
    "        results_dict = self.valid_ticker(results_dict)\n",
    "\n",
    "        results_copy = results_dict.copy()\n",
    "        try:\n",
    "            results_copy.popitem()\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "            print(\"None of the tickers are available in the Database.\")\n",
    "            return\n",
    "        else:\n",
    "            return self.dataframe_wrapper(results_dict, no_metrics, original_metrics)\n",
    "\n",
    "    @staticmethod\n",
    "    def isolate_timeseries(list_, metrics):\n",
    "        output_dict = defaultdict(dict)\n",
    "        size = len(list_)\n",
    "\n",
    "        for i in range(size):\n",
    "            try:\n",
    "                r = list_.pop()\n",
    "            except IndexError:\n",
    "                break\n",
    "            else:\n",
    "                dictionary = r['attributes'][0]\n",
    "                ticker = dictionary['expression'].split(',')\n",
    "                metric = ticker[-1][:-1]\n",
    "\n",
    "                ticker_split = ','.join(ticker[:-1])\n",
    "                ts_arr = np.array(dictionary['time-series'])\n",
    "\n",
    "                if ticker_split not in output_dict:\n",
    "                    output_dict[ticker_split]['real_date'] = ts_arr[:, 0]\n",
    "                    output_dict[ticker_split][metric] = ts_arr[:, 1]\n",
    "                else:\n",
    "                    output_dict[ticker_split][metric] = ts_arr[:, 1]\n",
    "\n",
    "        no_rows = ts_arr[:, 1].size\n",
    "        modified_dict = {}\n",
    "        d_frame_order = ['real_date'] + metrics\n",
    "\n",
    "        for k, v in output_dict.items():\n",
    "            arr = np.empty(shape=(no_rows, len(d_frame_order)), dtype=object)\n",
    "            for i, metric in enumerate(d_frame_order):\n",
    "                arr[:, i] = v[metric]\n",
    "\n",
    "            modified_dict[k] = arr\n",
    "        return modified_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def column_check(v, col):\n",
    "        returns = list(v[:, col])\n",
    "        condition = all([isinstance(elem, type(None)) for elem in returns])\n",
    "\n",
    "        return condition\n",
    "\n",
    "    def valid_ticker(self, _dict):\n",
    "        dict_copy = _dict.copy()\n",
    "        for k, v in _dict.items():\n",
    "\n",
    "            condition = self.column_check(v, 1)\n",
    "            if condition:\n",
    "                print(f\"The ticker, {k}, does not exist in the Database.\")\n",
    "                dict_copy.pop(k)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        return dict_copy\n",
    "\n",
    "    @staticmethod\n",
    "    def dataframe_wrapper(_dict, no_metrics, original_metrics):\n",
    "        tickers_no = len(_dict.keys())\n",
    "        length = list(_dict.values())[0].shape[0]\n",
    "\n",
    "        arr = np.empty(shape=(length * tickers_no, 3 + no_metrics), dtype=object)\n",
    "        print(arr.shape)\n",
    "\n",
    "        i = 0\n",
    "        for k, v in _dict.items():\n",
    "            ticker = k.split(',')\n",
    "            ticker = ticker[1].split('_')\n",
    "\n",
    "            cid = ticker[0]\n",
    "            xcat = '_'.join(ticker[1:])\n",
    "\n",
    "            cid_broad = np.repeat(cid, repeats=v.shape[0])\n",
    "            xcat_broad = np.repeat(xcat, repeats=v.shape[0])\n",
    "            data = np.column_stack((cid_broad, xcat_broad, v))\n",
    "\n",
    "            row = i * v.shape[0]\n",
    "            arr[row:row + v.shape[0], :] = data\n",
    "            i += 1\n",
    "\n",
    "        columns = ['cid', 'xcat', 'real_date']\n",
    "        cols_output = columns + original_metrics\n",
    "\n",
    "        df = pd.DataFrame(data=arr, columns=cols_output)\n",
    "\n",
    "        df['real_date'] = pd.to_datetime(df['real_date'], yearfirst=True)\n",
    "        df = df[df['real_date'].dt.dayofweek < 5]\n",
    "\n",
    "        return df.fillna(value=np.nan).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f0ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d95fc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dq_dataframe(tickers, metrics_=[\"value\"], start_d=\"2000-01-01\"):\n",
    "\n",
    "    unique_tickers = list(set(tickers))\n",
    "    dq_tickers = []\n",
    "    for metric in metrics_:\n",
    "        dq_tickers = dq_tickers + [\"DB(JPMAQS,\" + tick + f\",{metric})\"\n",
    "                                   for tick in unique_tickers]\n",
    "    print(f\"tickers: {tickers}\")\n",
    "\n",
    "    with open(\"config.yml\", 'r') as f:\n",
    "        cf = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    dq = DataQueryInterface(username=cf[\"dq\"][\"username\"], password=cf[\"dq\"][\"password\"],\n",
    "                            crt=\"api_macrosynergy_com.crt\",\n",
    "                            key=\"api_macrosynergy_com.key\")\n",
    "\n",
    "    df_ts = dq.get_tickers(tickers=dq_tickers, original_metrics=metrics,\n",
    "                                 start_date=start_d)\n",
    "\n",
    "    if isinstance(df_ts, pd.DataFrame):\n",
    "        df_ts = df_ts.sort_values(['cid', 'xcat', 'real_date']).reset_index(drop=True)\n",
    "\n",
    "        return df_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ded472f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of cross-sectional list: 32\n",
      "tickers: ['INR_CPIXFE_SJA_P6M6ML6AR', 'EUR_CPIXFE_SJA_P6M6ML6AR', 'GBP_CPIXFE_SJA_P6M6ML6AR', 'PEN_CPIXFE_SJA_P6M6ML6AR', 'CNY_CPIXFE_SJA_P6M6ML6AR', 'AUD_CPIXFE_SJA_P6M6ML6AR', 'KRW_CPIXFE_SJA_P6M6ML6AR', 'MYR_CPIXFE_SJA_P6M6ML6AR', 'ILS_CPIXFE_SJA_P6M6ML6AR', 'ZAR_CPIXFE_SJA_P6M6ML6AR', 'IDR_CPIXFE_SJA_P6M6ML6AR', 'SGD_CPIXFE_SJA_P6M6ML6AR', 'TRY_CPIXFE_SJA_P6M6ML6AR', 'RON_CPIXFE_SJA_P6M6ML6AR', 'NZD_CPIXFE_SJA_P6M6ML6AR', 'SEK_CPIXFE_SJA_P6M6ML6AR', 'USD_CPIXFE_SJA_P6M6ML6AR', 'RUB_CPIXFE_SJA_P6M6ML6AR', 'NOK_CPIXFE_SJA_P6M6ML6AR', 'PLN_CPIXFE_SJA_P6M6ML6AR', 'COP_CPIXFE_SJA_P6M6ML6AR', 'JPY_CPIXFE_SJA_P6M6ML6AR', 'THB_CPIXFE_SJA_P6M6ML6AR', 'TWD_CPIXFE_SJA_P6M6ML6AR', 'CHF_CPIXFE_SJA_P6M6ML6AR', 'MXN_CPIXFE_SJA_P6M6ML6AR', 'HKD_CPIXFE_SJA_P6M6ML6AR', 'PHP_CPIXFE_SJA_P6M6ML6AR', 'BRL_CPIXFE_SJA_P6M6ML6AR', 'CLP_CPIXFE_SJA_P6M6ML6AR', 'CAD_CPIXFE_SJA_P6M6ML6AR', 'HUF_CPIXFE_SJA_P6M6ML6AR']\n",
      "Number of metrics: 2.\n",
      "The ticker, DB(JPMAQS,HKD_CPIXFE_SJA_P6M6ML6AR, does not exist in the Database.\n",
      "The ticker, DB(JPMAQS,CLP_CPIXFE_SJA_P6M6ML6AR, does not exist in the Database.\n",
      "(118980, 5)\n",
      "Time taken: 126.64578914642334.\n",
      "       cid                  xcat  real_date     value  eop_lag\n",
      "0      AUD  CPIXFE_SJA_P6M6ML6AR 2011-01-03  3.330102     95.0\n",
      "1      AUD  CPIXFE_SJA_P6M6ML6AR 2011-01-04  3.330102     96.0\n",
      "2      AUD  CPIXFE_SJA_P6M6ML6AR 2011-01-05  3.235887     97.0\n",
      "3      AUD  CPIXFE_SJA_P6M6ML6AR 2011-01-06  3.235887     98.0\n",
      "4      AUD  CPIXFE_SJA_P6M6ML6AR 2011-01-07  3.235887     99.0\n",
      "...    ...                   ...        ...       ...      ...\n",
      "84955  ZAR  CPIXFE_SJA_P6M6ML6AR 2021-11-03  2.973670     34.0\n",
      "84956  ZAR  CPIXFE_SJA_P6M6ML6AR 2021-11-04       NaN      NaN\n",
      "84957  ZAR  CPIXFE_SJA_P6M6ML6AR 2021-11-05       NaN      NaN\n",
      "84958  ZAR  CPIXFE_SJA_P6M6ML6AR 2021-11-08       NaN      NaN\n",
      "84959  ZAR  CPIXFE_SJA_P6M6ML6AR 2021-11-09       NaN      NaN\n",
      "\n",
      "[84960 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "cids_dmca = ['AUD', 'CAD', 'CHF', 'EUR', 'GBP', 'JPY', 'NOK', 'NZD', 'SEK', 'USD']  # DM currency areas\n",
    "cids_dmec = ['DEM', 'ESP', 'FRF', 'ITL', 'NLG']  # DM euro area countries\n",
    "cids_latm = ['ARS', 'BRL', 'COP', 'CLP', 'MXN', 'PEN']  # Latam countries\n",
    "cids_emea = ['HUF', 'ILS', 'PLN', 'RON', 'RUB', 'TRY', 'ZAR']  # EMEA countries\n",
    "cids_emas = ['CNY', 'HKD', 'IDR', 'INR', 'KRW', 'MYR', 'PHP', 'SGD', 'THB', 'TWD']  # EM Asia countries\n",
    "cids_dm = cids_dmca + cids_dmec\n",
    "cids_em = cids_latm + cids_emea + cids_emas\n",
    "cids = sorted(cids_dm + cids_em)\n",
    "\n",
    "cids_eufx = ['CHF', 'HUF', 'NOK', 'PLN', 'RON', 'SEK']  # EUR benchmark\n",
    "cids_g2fx = ['GBP', 'RUB', 'TRY']  # dual benchmark\n",
    "cids_usfx = ['AUD', 'BRL', 'CAD', 'CLP', 'CNY', 'COP', 'EUR', 'IDR', 'ILS', 'INR', 'JPY', 'KRW', 'MYR',\n",
    "             'MXN', 'NZD', 'PEN', 'PHP', 'SGD', 'THB', 'TWD', 'ZAR']  # USD benchmark\n",
    "cids_fx = cids_usfx + cids_eufx + cids_g2fx\n",
    "\n",
    "metrics = ['value', 'eop_lag']\n",
    "\n",
    "cids = cids_dmca + cids_emea\n",
    "cids += cids_emas\n",
    "cids += cids_usfx\n",
    "cids = list(set(cids))\n",
    "\n",
    "print(f\"Length of cross-sectional list: {len(cids)}\")\n",
    "\n",
    "metrics = ['value', 'eop_lag']\n",
    "tix_1 = [cid + '_CPIXFE_SJA_P6M6ML6AR' for cid in cids]\n",
    "\n",
    "start = time.time()\n",
    "df = dq_dataframe(tix_1, metrics, \"2011-01-01\")\n",
    "end = time.time() - start\n",
    "print(f\"Time taken: {end}.\")\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
